{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PohCF88YwWgk"
   },
   "source": [
    "# Aufgaben Blatt 4 KI Machine Learning I\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9sieFkrwWgm"
   },
   "source": [
    "## Aufgabe 1 (Lineare Regression)\n",
    "\n",
    "Bearbeiten Sie die Aufgabe https://github.com/oduerr/ki/blob/main/linear_regression/lr_gradient_descent.ipynb\n",
    "\n",
    "Versuchen Sie den Code zu verstehen und machen die kleineren Aufgaben, die in dem notebook besprochen werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hr2BSdsNwWgm"
   },
   "source": [
    "## Aufgabe 2 (Titanic)\n",
    "In dieser Aufgabe nehmen Sie an der Titanic Challenge (https://www.kaggle.com/c/titanic) teil. Sie können die Aufgabe am eigenen PC lösen oder direkt in Kaggle lösen. Die Daten liegen auch auf Moodle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-_0tiX-wWgm"
   },
   "source": [
    "a) Lesen Sie die Trainingsdaten ein und teilen Sie sie in ein Validierungsdatenset (20%) und in ein eigentliches Trainigsdatenset (80%) auf. Finden Sie auf dem Validierungsdatensatz eine Regel für das Überleben alleine aufgrund der Klasse des Tickets (Pclass). Wenden Sie diese Regel auf die Validierungsdaten an. Wie gut ist die Genauigkeit (Anteil der korrekten Klassifikationen) auf den Validierungsdaten?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "PGm0-jEawWgn"
   },
   "outputs": [],
   "source": [
    "# Hinweise zum Einlesen\n",
    "import numpy as np\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val = pd.read_csv('titanic/train.csv')\n",
    "train, validate = train_test_split(train_val, test_size=0.2)\n",
    "\n",
    "# Hinweise zum Erzeugen einer Tabelle\n",
    "# pd.crosstab(...)\n",
    "\n",
    "# Hinweise um die Accuracy zu berechnen\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Survived\n",
      "Pclass          \n",
      "1       0.627119\n",
      "2       0.465278\n",
      "3       0.230179\n",
      "\n",
      "\n",
      "Total:  712\n",
      "survived:  268\n",
      "Predicted:  177\n",
      "Korrektheit (1 = 100%):  0.6604477611940298\n",
      "\n",
      "\n",
      "Total:  179\n",
      "survived:  74\n",
      "Predicted:  39\n",
      "Korrektheit (1 = 100%):  0.527027027027027\n"
     ]
    }
   ],
   "source": [
    "#print(train.head())\n",
    "\n",
    "trainSelected = train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=True).mean().sort_values(by='Survived', ascending=False)\n",
    "print(trainSelected)\n",
    "\n",
    "def simplePredict(set):\n",
    "    #print(set['Pclass'].tolist())\n",
    "    classes = set['Pclass'].tolist()\n",
    "    survived = []\n",
    "    survivedExists = True\n",
    "    try:\n",
    "        for s in set['Survived'].tolist():\n",
    "            if s == 1:\n",
    "                survived.append(s)\n",
    "    except BaseException:\n",
    "        survivedExists = False\n",
    "\n",
    "    person = set['PassengerId'].tolist()\n",
    "\n",
    "    survivedPred = []\n",
    "    for idx,pClass in enumerate(classes):\n",
    "        if (pClass == 1):\n",
    "            survivedPred.append(1) # person[idx])\n",
    "        else:\n",
    "            survivedPred.append(0)\n",
    "\n",
    "    print(\"\\n\\nTotal: \", len(person))\n",
    "    if survivedExists:\n",
    "        print(\"survived: \", len(survived))\n",
    "    print(\"Predicted: \", survivedPred.count(1))\n",
    "    if survivedExists:\n",
    "        print('Korrektheit (1 = 100%): ', survivedPred.count(1) / len(survived))\n",
    "\n",
    "    return survivedPred\n",
    "\n",
    "ex = simplePredict(train)\n",
    "ex = simplePredict(validate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAb6YDtTwWgn"
   },
   "source": [
    "b) Wenden Sie die Regel aus a) auf die Testdaten an und laden Sie Ihre Lösung hoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "SeplTB8SwWgo",
    "outputId": "98e2e1d5-0b85-4bb6-beb5-25e200b94913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Total:  418\n",
      "Predicted:  107\n",
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# Hinweise zum rausschreiben\n",
    "#pred_survived #Enhält die Predictions 0 für Tod 1 für Survived\n",
    "test_data = pd.read_csv('titanic/test.csv')\n",
    "\n",
    "pred_survived = simplePredict(test_data)\n",
    "\n",
    "output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': pred_survived})\n",
    "output.to_csv('my_submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0YqDhK3wWgp"
   },
   "source": [
    "c) Logistische Regression mit Pclass\n",
    "\n",
    "Trainieren Sie eine logistische Regression mit den Variablen 'Pclass'. Verwenden Sie die Klasse `sklearn.linear_model.LogisticRegression`. Berechnen Sie die Accuracy auf dem Validierungsset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6741573033707865"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# print(train_val.dtypes)\n",
    "\n",
    "train, validate = train_test_split(train_val, test_size=0.2)\n",
    "\n",
    "model = LogisticRegression()\n",
    "#model.add(sklearn.layers.Dense(20, activation='relu', input_shape=(10,)))\n",
    "\n",
    "model.fit(np.asarray(train['Pclass']).reshape(-1, 1), train['Survived'])\n",
    "# plt.plot(history)\n",
    "# plt.show()\n",
    "# model.evaluate(train['Pclass'], train['Survived'], verbose=2)\n",
    "\n",
    "pred_survived = model.predict(np.asarray(train['Pclass']).reshape(-1, 1))\n",
    "accuracy_score(np.asarray(train['Survived']).reshape(-1, 1), pred_survived)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-xC04XswWgp"
   },
   "source": [
    "d) Coding / Feature engineering \n",
    "\n",
    "d.i) Missing Values:\n",
    "\n",
    "Verwenden Sie nun weitere Features. Die Variable Age enthält Missing values, die Sie durch folgenden code ersetzen können (was passiert da?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "6i9qi9ywwWgq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.6109550561797753"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bei einem fehlenden Alter (NaN) wird der Median aller vorhandenen Alterszahlen berechnet (skips NULL values) und stattdessen eingesetzt\n",
    "validate[\"Age\"].fillna(train[\"Age\"].median(skipna=True), inplace=True)\n",
    "train[\"Age\"].fillna(train[\"Age\"].median(skipna=True), inplace=True)\n",
    "\n",
    "# Logistische Regression mit Age\n",
    "# model = LogisticRegression()\n",
    "model.fit(np.asarray(train['Age']).reshape(-1, 1), train['Survived'])\n",
    "pred_survived = model.predict(np.asarray(train['Age']).reshape(-1, 1))\n",
    "accuracy_score(np.asarray(train['Survived']).reshape(-1, 1), pred_survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxNwZco7wWgq"
   },
   "source": [
    "d.ii) Kategorische Variable\n",
    "\n",
    "Verwenden Sie die Funktion `pd.get_dummies` um die Variablen 'Pclass' and 'Sex' in numerische Werte umzuwandeln. Führen Sie nun eine logistische Regression durch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.776536312849162"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(pd.get_dummies(validate['Pclass']))\n",
    "# print(pd.get_dummies(validate['Sex']))\n",
    "\n",
    "# for train values\n",
    "pClassAsNum = pd.get_dummies(train['Pclass'])\n",
    "sexAsNum = pd.get_dummies(train['Sex'])\n",
    "train[\"Age\"].fillna(train[\"Age\"].median(skipna=True), inplace=True)\n",
    "\n",
    "model = LogisticRegression()\n",
    "toFit = pd.concat([pClassAsNum,sexAsNum, train[\"Age\"]], axis=1)\n",
    "model.fit(toFit, train['Survived'])\n",
    "\n",
    "# for validate values\n",
    "pClassAsNumVal = pd.get_dummies(validate['Pclass'])\n",
    "sexAsNumVal = pd.get_dummies(validate['Sex'])\n",
    "validate[\"Age\"].fillna(validate[\"Age\"].median(skipna=True), inplace=True)\n",
    "\n",
    "validate_values = pd.concat([pClassAsNumVal,sexAsNumVal, validate[\"Age\"]], axis=1)\n",
    "# print(validate_values)\n",
    "\n",
    "pred_survived = model.predict(validate_values)\n",
    "\n",
    "accuracy_score(np.asarray(validate['Survived']).reshape(-1, 1), pred_survived)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yu8UOcKAwWgq"
   },
   "source": [
    "e) Weitere Klassifikatoren. Neben der logistischen Regression, gibt es weitere Klassifikatoren. Der Random-Forest ist ein recht stabiler Klassifikator, was wäre die Performance von diesem Klassifikator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "v0eRJVGSwWgr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.8212290502793296"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hinweise zur Lösung\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "#rf hat nun gleiches interface, wie die logistische Regression\n",
    "\n",
    "# selbes Szenario wie aus d.ii) -> 0.81564245...\n",
    "\n",
    "toFit = pd.concat([pClassAsNum, sexAsNum, train[\"Age\"]], axis=1)\n",
    "rf.fit(toFit, train['Survived'])\n",
    "\n",
    "# for validate values\n",
    "pClassAsNumVal = pd.get_dummies(validate['Pclass'])\n",
    "sexAsNumVal = pd.get_dummies(validate['Sex'])\n",
    "validate[\"Age\"].fillna(validate[\"Age\"].median(skipna=True), inplace=True)\n",
    "\n",
    "validate_values = pd.concat([pClassAsNumVal,sexAsNumVal, validate[\"Age\"]], axis=1)\n",
    "# print(validate_values)\n",
    "\n",
    "pred_survived = rf.predict(validate_values)\n",
    "\n",
    "accuracy_score(np.asarray(validate['Survived']).reshape(-1, 1), pred_survived)\n",
    "\n",
    "\n",
    "\n",
    "#rf.fit(np.asarray(train['Pclass']).reshape(-1, 1), train['Survived'])\n",
    "#pred_survived = rf.predict(np.asarray(train['Pclass']).reshape(-1, 1))\n",
    "#accuracy_score(np.asarray(train['Survived']).reshape(-1, 1), pred_survived)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oe_oIy11wWgr"
   },
   "source": [
    "f) [optional] Versuchen Sie weitere Features zu erzeugen und laden den besten Klassifikator auf Kaggle hoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc49pWuewWgr"
   },
   "source": [
    "## Aufgabe 3 Titanic mit Neuronalen Netzen \n",
    "\n",
    "Hinweis: Diese Aufgabe kann erst nach der dritten Vorlesung in ML gemacht werden.\n",
    "\n",
    "Mit den gleichen Daten, wie in der Aufgabe 2 d. Erstellen Sie ein fully connected neural network und fitten es an die Ttrainingsdaten. Verwenden Sie mindestens zwei hidden Layer. Plotten Sie den Verlauf der Loss Kurve für die Trainings- und Validierungsdaten. Optional: Laden Sie Ihre beste Lösung auf Kaggle hoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "aYxfamTkwWgr",
    "outputId": "80a28e11-8256-4ab2-97ad-8f1d0ddda78e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "18/18 [==============================] - 1s 12ms/step - loss: 0.6723 - accuracy: 0.5923 - val_loss: 0.6274 - val_accuracy: 0.6993\n",
      "Epoch 2/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.6686 - accuracy: 0.5923 - val_loss: 0.6253 - val_accuracy: 0.7203\n",
      "Epoch 3/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.6675 - accuracy: 0.6098 - val_loss: 0.6190 - val_accuracy: 0.7203\n",
      "Epoch 4/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.6134 - val_loss: 0.6158 - val_accuracy: 0.7133\n",
      "Epoch 5/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.6555 - accuracy: 0.6116 - val_loss: 0.6093 - val_accuracy: 0.7063\n",
      "Epoch 6/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.6488 - accuracy: 0.6134 - val_loss: 0.6069 - val_accuracy: 0.7063\n",
      "Epoch 7/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.6420 - accuracy: 0.6221 - val_loss: 0.5883 - val_accuracy: 0.7063\n",
      "Epoch 8/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.6349 - accuracy: 0.6309 - val_loss: 0.5888 - val_accuracy: 0.7273\n",
      "Epoch 9/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.6188 - accuracy: 0.6362 - val_loss: 0.5694 - val_accuracy: 0.7413\n",
      "Epoch 10/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.6044 - accuracy: 0.7346 - val_loss: 0.5533 - val_accuracy: 0.7972\n",
      "Epoch 11/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.5874 - accuracy: 0.7065 - val_loss: 0.5364 - val_accuracy: 0.8531\n",
      "Epoch 12/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5700 - accuracy: 0.7821 - val_loss: 0.5096 - val_accuracy: 0.8531\n",
      "Epoch 13/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.5538 - accuracy: 0.7733 - val_loss: 0.5116 - val_accuracy: 0.8322\n",
      "Epoch 14/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5418 - accuracy: 0.7786 - val_loss: 0.4896 - val_accuracy: 0.8462\n",
      "Epoch 15/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5247 - accuracy: 0.8049 - val_loss: 0.4655 - val_accuracy: 0.8462\n",
      "Epoch 16/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.5176 - accuracy: 0.7838 - val_loss: 0.4533 - val_accuracy: 0.8462\n",
      "Epoch 17/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.5038 - accuracy: 0.7996 - val_loss: 0.4491 - val_accuracy: 0.8322\n",
      "Epoch 18/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4973 - accuracy: 0.7979 - val_loss: 0.4386 - val_accuracy: 0.8462\n",
      "Epoch 19/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4903 - accuracy: 0.8014 - val_loss: 0.4406 - val_accuracy: 0.8322\n",
      "Epoch 20/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4884 - accuracy: 0.7926 - val_loss: 0.4282 - val_accuracy: 0.8322\n",
      "Epoch 21/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4791 - accuracy: 0.8014 - val_loss: 0.4272 - val_accuracy: 0.8392\n",
      "Epoch 22/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4767 - accuracy: 0.8049 - val_loss: 0.4285 - val_accuracy: 0.8252\n",
      "Epoch 23/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4743 - accuracy: 0.7944 - val_loss: 0.4200 - val_accuracy: 0.8322\n",
      "Epoch 24/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4713 - accuracy: 0.8067 - val_loss: 0.4209 - val_accuracy: 0.8252\n",
      "Epoch 25/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4692 - accuracy: 0.7996 - val_loss: 0.4178 - val_accuracy: 0.8392\n",
      "Epoch 26/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4684 - accuracy: 0.7979 - val_loss: 0.4232 - val_accuracy: 0.8252\n",
      "Epoch 27/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4665 - accuracy: 0.7891 - val_loss: 0.4157 - val_accuracy: 0.8252\n",
      "Epoch 28/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4668 - accuracy: 0.8014 - val_loss: 0.4201 - val_accuracy: 0.8252\n",
      "Epoch 29/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4663 - accuracy: 0.8014 - val_loss: 0.4138 - val_accuracy: 0.8252\n",
      "Epoch 30/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4645 - accuracy: 0.7891 - val_loss: 0.4129 - val_accuracy: 0.8252\n",
      "Epoch 31/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4648 - accuracy: 0.7979 - val_loss: 0.4154 - val_accuracy: 0.8252\n",
      "Epoch 32/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4621 - accuracy: 0.7909 - val_loss: 0.4122 - val_accuracy: 0.8252\n",
      "Epoch 33/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4617 - accuracy: 0.7873 - val_loss: 0.4087 - val_accuracy: 0.8252\n",
      "Epoch 34/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4604 - accuracy: 0.7979 - val_loss: 0.4155 - val_accuracy: 0.8252\n",
      "Epoch 35/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4600 - accuracy: 0.7891 - val_loss: 0.4076 - val_accuracy: 0.8252\n",
      "Epoch 36/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4616 - accuracy: 0.7944 - val_loss: 0.4153 - val_accuracy: 0.8252\n",
      "Epoch 37/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4571 - accuracy: 0.8032 - val_loss: 0.4070 - val_accuracy: 0.8252\n",
      "Epoch 38/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4583 - accuracy: 0.7944 - val_loss: 0.4120 - val_accuracy: 0.8252\n",
      "Epoch 39/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4580 - accuracy: 0.7996 - val_loss: 0.4092 - val_accuracy: 0.8252\n",
      "Epoch 40/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4577 - accuracy: 0.7996 - val_loss: 0.4103 - val_accuracy: 0.8252\n",
      "Epoch 41/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4556 - accuracy: 0.8014 - val_loss: 0.4069 - val_accuracy: 0.8252\n",
      "Epoch 42/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4561 - accuracy: 0.7926 - val_loss: 0.4057 - val_accuracy: 0.8252\n",
      "Epoch 43/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4567 - accuracy: 0.7944 - val_loss: 0.4094 - val_accuracy: 0.8252\n",
      "Epoch 44/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4548 - accuracy: 0.7891 - val_loss: 0.4048 - val_accuracy: 0.8392\n",
      "Epoch 45/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4563 - accuracy: 0.7961 - val_loss: 0.4099 - val_accuracy: 0.8252\n",
      "Epoch 46/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4567 - accuracy: 0.7944 - val_loss: 0.4079 - val_accuracy: 0.8322\n",
      "Epoch 47/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4542 - accuracy: 0.7873 - val_loss: 0.4038 - val_accuracy: 0.8392\n",
      "Epoch 48/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4555 - accuracy: 0.8032 - val_loss: 0.4099 - val_accuracy: 0.8252\n",
      "Epoch 49/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4553 - accuracy: 0.7909 - val_loss: 0.4032 - val_accuracy: 0.8392\n",
      "Epoch 50/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4529 - accuracy: 0.8014 - val_loss: 0.4065 - val_accuracy: 0.8252\n",
      "Epoch 51/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4563 - accuracy: 0.7891 - val_loss: 0.4013 - val_accuracy: 0.8462\n",
      "Epoch 52/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4576 - accuracy: 0.7856 - val_loss: 0.4025 - val_accuracy: 0.8392\n",
      "Epoch 53/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4531 - accuracy: 0.7996 - val_loss: 0.4062 - val_accuracy: 0.8252\n",
      "Epoch 54/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4530 - accuracy: 0.7961 - val_loss: 0.4041 - val_accuracy: 0.8252\n",
      "Epoch 55/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4535 - accuracy: 0.8014 - val_loss: 0.4073 - val_accuracy: 0.8252\n",
      "Epoch 56/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4511 - accuracy: 0.8014 - val_loss: 0.4024 - val_accuracy: 0.8252\n",
      "Epoch 57/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4499 - accuracy: 0.8014 - val_loss: 0.4083 - val_accuracy: 0.8182\n",
      "Epoch 58/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4512 - accuracy: 0.7891 - val_loss: 0.4005 - val_accuracy: 0.8392\n",
      "Epoch 59/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4503 - accuracy: 0.7909 - val_loss: 0.4008 - val_accuracy: 0.8252\n",
      "Epoch 60/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4498 - accuracy: 0.8014 - val_loss: 0.4009 - val_accuracy: 0.8252\n",
      "Epoch 61/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4489 - accuracy: 0.8032 - val_loss: 0.4069 - val_accuracy: 0.8252\n",
      "Epoch 62/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4504 - accuracy: 0.8067 - val_loss: 0.4047 - val_accuracy: 0.8252\n",
      "Epoch 63/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4486 - accuracy: 0.8032 - val_loss: 0.4020 - val_accuracy: 0.8252\n",
      "Epoch 64/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4504 - accuracy: 0.7996 - val_loss: 0.4073 - val_accuracy: 0.8182\n",
      "Epoch 65/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4493 - accuracy: 0.7856 - val_loss: 0.3976 - val_accuracy: 0.8462\n",
      "Epoch 66/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4488 - accuracy: 0.7979 - val_loss: 0.4037 - val_accuracy: 0.8322\n",
      "Epoch 67/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4478 - accuracy: 0.7856 - val_loss: 0.4001 - val_accuracy: 0.8392\n",
      "Epoch 68/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4516 - accuracy: 0.7944 - val_loss: 0.4017 - val_accuracy: 0.8252\n",
      "Epoch 69/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4481 - accuracy: 0.7909 - val_loss: 0.4010 - val_accuracy: 0.8252\n",
      "Epoch 70/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4471 - accuracy: 0.8032 - val_loss: 0.3980 - val_accuracy: 0.8462\n",
      "Epoch 71/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4470 - accuracy: 0.7961 - val_loss: 0.4037 - val_accuracy: 0.8252\n",
      "Epoch 72/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4478 - accuracy: 0.7944 - val_loss: 0.3986 - val_accuracy: 0.8392\n",
      "Epoch 73/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4472 - accuracy: 0.7996 - val_loss: 0.3994 - val_accuracy: 0.8252\n",
      "Epoch 74/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4474 - accuracy: 0.8014 - val_loss: 0.3984 - val_accuracy: 0.8392\n",
      "Epoch 75/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4483 - accuracy: 0.7926 - val_loss: 0.4004 - val_accuracy: 0.8252\n",
      "Epoch 76/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4503 - accuracy: 0.7979 - val_loss: 0.3972 - val_accuracy: 0.8462\n",
      "Epoch 77/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4540 - accuracy: 0.7733 - val_loss: 0.3967 - val_accuracy: 0.8462\n",
      "Epoch 78/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4509 - accuracy: 0.7926 - val_loss: 0.3997 - val_accuracy: 0.8252\n",
      "Epoch 79/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4447 - accuracy: 0.7944 - val_loss: 0.4018 - val_accuracy: 0.8252\n",
      "Epoch 80/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4438 - accuracy: 0.8014 - val_loss: 0.3971 - val_accuracy: 0.8462\n",
      "Epoch 81/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4445 - accuracy: 0.7996 - val_loss: 0.4020 - val_accuracy: 0.8322\n",
      "Epoch 82/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4453 - accuracy: 0.7926 - val_loss: 0.3948 - val_accuracy: 0.8462\n",
      "Epoch 83/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4495 - accuracy: 0.7891 - val_loss: 0.3970 - val_accuracy: 0.8392\n",
      "Epoch 84/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4445 - accuracy: 0.8049 - val_loss: 0.3974 - val_accuracy: 0.8392\n",
      "Epoch 85/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4439 - accuracy: 0.7926 - val_loss: 0.3979 - val_accuracy: 0.8252\n",
      "Epoch 86/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4447 - accuracy: 0.7926 - val_loss: 0.3945 - val_accuracy: 0.8462\n",
      "Epoch 87/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4426 - accuracy: 0.7926 - val_loss: 0.3988 - val_accuracy: 0.8252\n",
      "Epoch 88/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4462 - accuracy: 0.7961 - val_loss: 0.3953 - val_accuracy: 0.8462\n",
      "Epoch 89/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4433 - accuracy: 0.7873 - val_loss: 0.3950 - val_accuracy: 0.8462\n",
      "Epoch 90/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4428 - accuracy: 0.7909 - val_loss: 0.3999 - val_accuracy: 0.8252\n",
      "Epoch 91/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4421 - accuracy: 0.8032 - val_loss: 0.3960 - val_accuracy: 0.8392\n",
      "Epoch 92/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4420 - accuracy: 0.8032 - val_loss: 0.3960 - val_accuracy: 0.8392\n",
      "Epoch 93/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4472 - accuracy: 0.7838 - val_loss: 0.3983 - val_accuracy: 0.8252\n",
      "Epoch 94/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4436 - accuracy: 0.7891 - val_loss: 0.3958 - val_accuracy: 0.8462\n",
      "Epoch 95/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4456 - accuracy: 0.7838 - val_loss: 0.3938 - val_accuracy: 0.8462\n",
      "Epoch 96/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4442 - accuracy: 0.7961 - val_loss: 0.3968 - val_accuracy: 0.8392\n",
      "Epoch 97/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4410 - accuracy: 0.7873 - val_loss: 0.3957 - val_accuracy: 0.8462\n",
      "Epoch 98/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4438 - accuracy: 0.8014 - val_loss: 0.3936 - val_accuracy: 0.8462\n",
      "Epoch 99/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4424 - accuracy: 0.7979 - val_loss: 0.3944 - val_accuracy: 0.8462\n",
      "Epoch 100/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4415 - accuracy: 0.7821 - val_loss: 0.3978 - val_accuracy: 0.8252\n",
      "Epoch 101/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4418 - accuracy: 0.8032 - val_loss: 0.3931 - val_accuracy: 0.8462\n",
      "Epoch 102/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4418 - accuracy: 0.7873 - val_loss: 0.3973 - val_accuracy: 0.8252\n",
      "Epoch 103/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4414 - accuracy: 0.7979 - val_loss: 0.3976 - val_accuracy: 0.8252\n",
      "Epoch 104/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4398 - accuracy: 0.8049 - val_loss: 0.3945 - val_accuracy: 0.8462\n",
      "Epoch 105/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4405 - accuracy: 0.7979 - val_loss: 0.3932 - val_accuracy: 0.8462\n",
      "Epoch 106/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4409 - accuracy: 0.7873 - val_loss: 0.4003 - val_accuracy: 0.8252\n",
      "Epoch 107/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4415 - accuracy: 0.7838 - val_loss: 0.3947 - val_accuracy: 0.8462\n",
      "Epoch 108/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4389 - accuracy: 0.7926 - val_loss: 0.3942 - val_accuracy: 0.8462\n",
      "Epoch 109/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4403 - accuracy: 0.7979 - val_loss: 0.3960 - val_accuracy: 0.8252\n",
      "Epoch 110/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4420 - accuracy: 0.7926 - val_loss: 0.3963 - val_accuracy: 0.8252\n",
      "Epoch 111/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4389 - accuracy: 0.8049 - val_loss: 0.3945 - val_accuracy: 0.8462\n",
      "Epoch 112/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4400 - accuracy: 0.7821 - val_loss: 0.3948 - val_accuracy: 0.8462\n",
      "Epoch 113/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4399 - accuracy: 0.8014 - val_loss: 0.3933 - val_accuracy: 0.8462\n",
      "Epoch 114/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4395 - accuracy: 0.7873 - val_loss: 0.3929 - val_accuracy: 0.8462\n",
      "Epoch 115/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4478 - accuracy: 0.7750 - val_loss: 0.3917 - val_accuracy: 0.8531\n",
      "Epoch 116/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4416 - accuracy: 0.7909 - val_loss: 0.3960 - val_accuracy: 0.8392\n",
      "Epoch 117/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4414 - accuracy: 0.7873 - val_loss: 0.4003 - val_accuracy: 0.8252\n",
      "Epoch 118/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4416 - accuracy: 0.7961 - val_loss: 0.3919 - val_accuracy: 0.8462\n",
      "Epoch 119/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4398 - accuracy: 0.7979 - val_loss: 0.3985 - val_accuracy: 0.8252\n",
      "Epoch 120/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4378 - accuracy: 0.7996 - val_loss: 0.3926 - val_accuracy: 0.8462\n",
      "Epoch 121/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4378 - accuracy: 0.7979 - val_loss: 0.3954 - val_accuracy: 0.8252\n",
      "Epoch 122/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4409 - accuracy: 0.7891 - val_loss: 0.3931 - val_accuracy: 0.8462\n",
      "Epoch 123/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4387 - accuracy: 0.7944 - val_loss: 0.3978 - val_accuracy: 0.8322\n",
      "Epoch 124/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4382 - accuracy: 0.7856 - val_loss: 0.3941 - val_accuracy: 0.8462\n",
      "Epoch 125/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4383 - accuracy: 0.7996 - val_loss: 0.3951 - val_accuracy: 0.8462\n",
      "Epoch 126/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4370 - accuracy: 0.8067 - val_loss: 0.3923 - val_accuracy: 0.8462\n",
      "Epoch 127/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4369 - accuracy: 0.7873 - val_loss: 0.3930 - val_accuracy: 0.8462\n",
      "Epoch 128/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4403 - accuracy: 0.7821 - val_loss: 0.3909 - val_accuracy: 0.8462\n",
      "Epoch 129/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4393 - accuracy: 0.7786 - val_loss: 0.3961 - val_accuracy: 0.8252\n",
      "Epoch 130/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4372 - accuracy: 0.7996 - val_loss: 0.3922 - val_accuracy: 0.8462\n",
      "Epoch 131/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4364 - accuracy: 0.7891 - val_loss: 0.3957 - val_accuracy: 0.8252\n",
      "Epoch 132/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4417 - accuracy: 0.7768 - val_loss: 0.3908 - val_accuracy: 0.8531\n",
      "Epoch 133/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4377 - accuracy: 0.7856 - val_loss: 0.3964 - val_accuracy: 0.8252\n",
      "Epoch 134/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4367 - accuracy: 0.8014 - val_loss: 0.3921 - val_accuracy: 0.8462\n",
      "Epoch 135/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4357 - accuracy: 0.7961 - val_loss: 0.3963 - val_accuracy: 0.8252\n",
      "Epoch 136/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4404 - accuracy: 0.7891 - val_loss: 0.3916 - val_accuracy: 0.8462\n",
      "Epoch 137/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4371 - accuracy: 0.7926 - val_loss: 0.3950 - val_accuracy: 0.8252\n",
      "Epoch 138/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4366 - accuracy: 0.7856 - val_loss: 0.3932 - val_accuracy: 0.8462\n",
      "Epoch 139/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4371 - accuracy: 0.7891 - val_loss: 0.3963 - val_accuracy: 0.8252\n",
      "Epoch 140/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4381 - accuracy: 0.7944 - val_loss: 0.3955 - val_accuracy: 0.8392\n",
      "Epoch 141/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4374 - accuracy: 0.7944 - val_loss: 0.3923 - val_accuracy: 0.8462\n",
      "Epoch 142/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4373 - accuracy: 0.7873 - val_loss: 0.3931 - val_accuracy: 0.8462\n",
      "Epoch 143/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4355 - accuracy: 0.8032 - val_loss: 0.3956 - val_accuracy: 0.8252\n",
      "Epoch 144/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4355 - accuracy: 0.7979 - val_loss: 0.3920 - val_accuracy: 0.8462\n",
      "Epoch 145/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4355 - accuracy: 0.7979 - val_loss: 0.3960 - val_accuracy: 0.8252\n",
      "Epoch 146/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4359 - accuracy: 0.8032 - val_loss: 0.3931 - val_accuracy: 0.8462\n",
      "Epoch 147/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4412 - accuracy: 0.7909 - val_loss: 0.3904 - val_accuracy: 0.8531\n",
      "Epoch 148/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4354 - accuracy: 0.7996 - val_loss: 0.3955 - val_accuracy: 0.8252\n",
      "Epoch 149/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4369 - accuracy: 0.7979 - val_loss: 0.3933 - val_accuracy: 0.8322\n",
      "Epoch 150/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4363 - accuracy: 0.7944 - val_loss: 0.3931 - val_accuracy: 0.8462\n",
      "Epoch 151/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4352 - accuracy: 0.7979 - val_loss: 0.3915 - val_accuracy: 0.8462\n",
      "Epoch 152/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4344 - accuracy: 0.7909 - val_loss: 0.3923 - val_accuracy: 0.8462\n",
      "Epoch 153/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4352 - accuracy: 0.7979 - val_loss: 0.3930 - val_accuracy: 0.8462\n",
      "Epoch 154/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4345 - accuracy: 0.7873 - val_loss: 0.3941 - val_accuracy: 0.8252\n",
      "Epoch 155/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4371 - accuracy: 0.7873 - val_loss: 0.3921 - val_accuracy: 0.8462\n",
      "Epoch 156/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4381 - accuracy: 0.7891 - val_loss: 0.3933 - val_accuracy: 0.8462\n",
      "Epoch 157/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4345 - accuracy: 0.8032 - val_loss: 0.3941 - val_accuracy: 0.8462\n",
      "Epoch 158/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4357 - accuracy: 0.7891 - val_loss: 0.3916 - val_accuracy: 0.8462\n",
      "Epoch 159/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4348 - accuracy: 0.8032 - val_loss: 0.3932 - val_accuracy: 0.8462\n",
      "Epoch 160/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4348 - accuracy: 0.8032 - val_loss: 0.3931 - val_accuracy: 0.8462\n",
      "Epoch 161/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4374 - accuracy: 0.7786 - val_loss: 0.3969 - val_accuracy: 0.8252\n",
      "Epoch 162/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4348 - accuracy: 0.7961 - val_loss: 0.3922 - val_accuracy: 0.8462\n",
      "Epoch 163/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4338 - accuracy: 0.7996 - val_loss: 0.3937 - val_accuracy: 0.8462\n",
      "Epoch 164/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4341 - accuracy: 0.7856 - val_loss: 0.3934 - val_accuracy: 0.8462\n",
      "Epoch 165/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4340 - accuracy: 0.7909 - val_loss: 0.3924 - val_accuracy: 0.8462\n",
      "Epoch 166/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4371 - accuracy: 0.7733 - val_loss: 0.3916 - val_accuracy: 0.8462\n",
      "Epoch 167/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4385 - accuracy: 0.7856 - val_loss: 0.3928 - val_accuracy: 0.8462\n",
      "Epoch 168/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4337 - accuracy: 0.8067 - val_loss: 0.3950 - val_accuracy: 0.8462\n",
      "Epoch 169/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4342 - accuracy: 0.7961 - val_loss: 0.3939 - val_accuracy: 0.8462\n",
      "Epoch 170/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4330 - accuracy: 0.8032 - val_loss: 0.3958 - val_accuracy: 0.8252\n",
      "Epoch 171/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4324 - accuracy: 0.8049 - val_loss: 0.3923 - val_accuracy: 0.8462\n",
      "Epoch 172/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4344 - accuracy: 0.7909 - val_loss: 0.3929 - val_accuracy: 0.8462\n",
      "Epoch 173/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4344 - accuracy: 0.7979 - val_loss: 0.3936 - val_accuracy: 0.8462\n",
      "Epoch 174/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4343 - accuracy: 0.7944 - val_loss: 0.3923 - val_accuracy: 0.8462\n",
      "Epoch 175/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4342 - accuracy: 0.7891 - val_loss: 0.3937 - val_accuracy: 0.8392\n",
      "Epoch 176/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4332 - accuracy: 0.8049 - val_loss: 0.3926 - val_accuracy: 0.8462\n",
      "Epoch 177/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4345 - accuracy: 0.7979 - val_loss: 0.3917 - val_accuracy: 0.8462\n",
      "Epoch 178/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4352 - accuracy: 0.7961 - val_loss: 0.3916 - val_accuracy: 0.8462\n",
      "Epoch 179/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4336 - accuracy: 0.7996 - val_loss: 0.3921 - val_accuracy: 0.8462\n",
      "Epoch 180/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4343 - accuracy: 0.7926 - val_loss: 0.3945 - val_accuracy: 0.8252\n",
      "Epoch 181/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4351 - accuracy: 0.7979 - val_loss: 0.3947 - val_accuracy: 0.8252\n",
      "Epoch 182/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4336 - accuracy: 0.7979 - val_loss: 0.3924 - val_accuracy: 0.8462\n",
      "Epoch 183/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4348 - accuracy: 0.7891 - val_loss: 0.3951 - val_accuracy: 0.8392\n",
      "Epoch 184/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4386 - accuracy: 0.7944 - val_loss: 0.3908 - val_accuracy: 0.8462\n",
      "Epoch 185/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4317 - accuracy: 0.8049 - val_loss: 0.3999 - val_accuracy: 0.8252\n",
      "Epoch 186/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4329 - accuracy: 0.7909 - val_loss: 0.3916 - val_accuracy: 0.8462\n",
      "Epoch 187/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4330 - accuracy: 0.7961 - val_loss: 0.3940 - val_accuracy: 0.8252\n",
      "Epoch 188/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4329 - accuracy: 0.7961 - val_loss: 0.3904 - val_accuracy: 0.8462\n",
      "Epoch 189/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4321 - accuracy: 0.8049 - val_loss: 0.3957 - val_accuracy: 0.8252\n",
      "Epoch 190/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4334 - accuracy: 0.7996 - val_loss: 0.3909 - val_accuracy: 0.8462\n",
      "Epoch 191/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4317 - accuracy: 0.8032 - val_loss: 0.3935 - val_accuracy: 0.8392\n",
      "Epoch 192/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4320 - accuracy: 0.7996 - val_loss: 0.3923 - val_accuracy: 0.8462\n",
      "Epoch 193/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4314 - accuracy: 0.7979 - val_loss: 0.3959 - val_accuracy: 0.8252\n",
      "Epoch 194/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4333 - accuracy: 0.8014 - val_loss: 0.3909 - val_accuracy: 0.8462\n",
      "Epoch 195/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4334 - accuracy: 0.7891 - val_loss: 0.3929 - val_accuracy: 0.8462\n",
      "Epoch 196/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4342 - accuracy: 0.7873 - val_loss: 0.3979 - val_accuracy: 0.8252\n",
      "Epoch 197/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4333 - accuracy: 0.7961 - val_loss: 0.3939 - val_accuracy: 0.8462\n",
      "Epoch 198/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4323 - accuracy: 0.7996 - val_loss: 0.3928 - val_accuracy: 0.8462\n",
      "Epoch 199/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4330 - accuracy: 0.7838 - val_loss: 0.3927 - val_accuracy: 0.8462\n",
      "Epoch 200/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4313 - accuracy: 0.8014 - val_loss: 0.3941 - val_accuracy: 0.8392\n",
      "Epoch 201/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4319 - accuracy: 0.7979 - val_loss: 0.3918 - val_accuracy: 0.8462\n",
      "Epoch 202/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4339 - accuracy: 0.7979 - val_loss: 0.3915 - val_accuracy: 0.8462\n",
      "Epoch 203/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4314 - accuracy: 0.7873 - val_loss: 0.3921 - val_accuracy: 0.8462\n",
      "Epoch 204/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4328 - accuracy: 0.7944 - val_loss: 0.3926 - val_accuracy: 0.8462\n",
      "Epoch 205/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4316 - accuracy: 0.7979 - val_loss: 0.3920 - val_accuracy: 0.8462\n",
      "Epoch 206/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4337 - accuracy: 0.7909 - val_loss: 0.3925 - val_accuracy: 0.8462\n",
      "Epoch 207/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4323 - accuracy: 0.7873 - val_loss: 0.3927 - val_accuracy: 0.8462\n",
      "Epoch 208/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4319 - accuracy: 0.8014 - val_loss: 0.3946 - val_accuracy: 0.8392\n",
      "Epoch 209/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4312 - accuracy: 0.7909 - val_loss: 0.3924 - val_accuracy: 0.8462\n",
      "Epoch 210/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4325 - accuracy: 0.7926 - val_loss: 0.3918 - val_accuracy: 0.8462\n",
      "Epoch 211/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4369 - accuracy: 0.7768 - val_loss: 0.3976 - val_accuracy: 0.8252\n",
      "Epoch 212/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4350 - accuracy: 0.7821 - val_loss: 0.3918 - val_accuracy: 0.8462\n",
      "Epoch 213/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4328 - accuracy: 0.7979 - val_loss: 0.3932 - val_accuracy: 0.8462\n",
      "Epoch 214/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4315 - accuracy: 0.8014 - val_loss: 0.3945 - val_accuracy: 0.8252\n",
      "Epoch 215/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4322 - accuracy: 0.7926 - val_loss: 0.3923 - val_accuracy: 0.8462\n",
      "Epoch 216/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4328 - accuracy: 0.8014 - val_loss: 0.3923 - val_accuracy: 0.8462\n",
      "Epoch 217/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4323 - accuracy: 0.7803 - val_loss: 0.3933 - val_accuracy: 0.8462\n",
      "Epoch 218/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4321 - accuracy: 0.7979 - val_loss: 0.3937 - val_accuracy: 0.8462\n",
      "Epoch 219/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4324 - accuracy: 0.8014 - val_loss: 0.3950 - val_accuracy: 0.8392\n",
      "Epoch 220/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4308 - accuracy: 0.7856 - val_loss: 0.3921 - val_accuracy: 0.8462\n",
      "Epoch 221/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4307 - accuracy: 0.8102 - val_loss: 0.3951 - val_accuracy: 0.8252\n",
      "Epoch 222/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4320 - accuracy: 0.7856 - val_loss: 0.3924 - val_accuracy: 0.8462\n",
      "Epoch 223/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4340 - accuracy: 0.7961 - val_loss: 0.3917 - val_accuracy: 0.8462\n",
      "Epoch 224/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4338 - accuracy: 0.8032 - val_loss: 0.4006 - val_accuracy: 0.8252\n",
      "Epoch 225/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4382 - accuracy: 0.7821 - val_loss: 0.3910 - val_accuracy: 0.8601\n",
      "Epoch 226/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4307 - accuracy: 0.7961 - val_loss: 0.4009 - val_accuracy: 0.8252\n",
      "Epoch 227/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4330 - accuracy: 0.7803 - val_loss: 0.3913 - val_accuracy: 0.8462\n",
      "Epoch 228/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4312 - accuracy: 0.7979 - val_loss: 0.3941 - val_accuracy: 0.8462\n",
      "Epoch 229/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4336 - accuracy: 0.7944 - val_loss: 0.3914 - val_accuracy: 0.8462\n",
      "Epoch 230/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4317 - accuracy: 0.7926 - val_loss: 0.3956 - val_accuracy: 0.8392\n",
      "Epoch 231/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4318 - accuracy: 0.7996 - val_loss: 0.3921 - val_accuracy: 0.8462\n",
      "Epoch 232/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4320 - accuracy: 0.7891 - val_loss: 0.3934 - val_accuracy: 0.8462\n",
      "Epoch 233/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4302 - accuracy: 0.7979 - val_loss: 0.3941 - val_accuracy: 0.8392\n",
      "Epoch 234/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4303 - accuracy: 0.7979 - val_loss: 0.3926 - val_accuracy: 0.8462\n",
      "Epoch 235/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4311 - accuracy: 0.7979 - val_loss: 0.3923 - val_accuracy: 0.8462\n",
      "Epoch 236/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4304 - accuracy: 0.7961 - val_loss: 0.3947 - val_accuracy: 0.8462\n",
      "Epoch 237/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4329 - accuracy: 0.7961 - val_loss: 0.3917 - val_accuracy: 0.8462\n",
      "Epoch 238/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4301 - accuracy: 0.7996 - val_loss: 0.3980 - val_accuracy: 0.8252\n",
      "Epoch 239/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4301 - accuracy: 0.8120 - val_loss: 0.3924 - val_accuracy: 0.8462\n",
      "Epoch 240/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4302 - accuracy: 0.7996 - val_loss: 0.3966 - val_accuracy: 0.8252\n",
      "Epoch 241/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4309 - accuracy: 0.7996 - val_loss: 0.3940 - val_accuracy: 0.8462\n",
      "Epoch 242/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4337 - accuracy: 0.7891 - val_loss: 0.3954 - val_accuracy: 0.8252\n",
      "Epoch 243/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4342 - accuracy: 0.7909 - val_loss: 0.3947 - val_accuracy: 0.8462\n",
      "Epoch 244/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4323 - accuracy: 0.8049 - val_loss: 0.3932 - val_accuracy: 0.8462\n",
      "Epoch 245/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4301 - accuracy: 0.7979 - val_loss: 0.3954 - val_accuracy: 0.8252\n",
      "Epoch 246/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4300 - accuracy: 0.8032 - val_loss: 0.3944 - val_accuracy: 0.8462\n",
      "Epoch 247/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4315 - accuracy: 0.8032 - val_loss: 0.3945 - val_accuracy: 0.8462\n",
      "Epoch 248/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4300 - accuracy: 0.7891 - val_loss: 0.3947 - val_accuracy: 0.8462\n",
      "Epoch 249/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4297 - accuracy: 0.8014 - val_loss: 0.3934 - val_accuracy: 0.8462\n",
      "Epoch 250/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4297 - accuracy: 0.7873 - val_loss: 0.3942 - val_accuracy: 0.8392\n",
      "Epoch 251/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4314 - accuracy: 0.7821 - val_loss: 0.3935 - val_accuracy: 0.8462\n",
      "Epoch 252/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4306 - accuracy: 0.7944 - val_loss: 0.3929 - val_accuracy: 0.8462\n",
      "Epoch 253/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4302 - accuracy: 0.8067 - val_loss: 0.3954 - val_accuracy: 0.8252\n",
      "Epoch 254/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4299 - accuracy: 0.7979 - val_loss: 0.3938 - val_accuracy: 0.8462\n",
      "Epoch 255/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4302 - accuracy: 0.7979 - val_loss: 0.3961 - val_accuracy: 0.8252\n",
      "Epoch 256/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4303 - accuracy: 0.7996 - val_loss: 0.3935 - val_accuracy: 0.8462\n",
      "Epoch 257/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4307 - accuracy: 0.8049 - val_loss: 0.3938 - val_accuracy: 0.8462\n",
      "Epoch 258/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4329 - accuracy: 0.7873 - val_loss: 0.3952 - val_accuracy: 0.8252\n",
      "Epoch 259/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4313 - accuracy: 0.7909 - val_loss: 0.3917 - val_accuracy: 0.8531\n",
      "Epoch 260/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4320 - accuracy: 0.7873 - val_loss: 0.3959 - val_accuracy: 0.8252\n",
      "Epoch 261/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4329 - accuracy: 0.7873 - val_loss: 0.3958 - val_accuracy: 0.8252\n",
      "Epoch 262/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4299 - accuracy: 0.7979 - val_loss: 0.3934 - val_accuracy: 0.8462\n",
      "Epoch 263/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4307 - accuracy: 0.7979 - val_loss: 0.3940 - val_accuracy: 0.8462\n",
      "Epoch 264/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4320 - accuracy: 0.7856 - val_loss: 0.3954 - val_accuracy: 0.8392\n",
      "Epoch 265/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4316 - accuracy: 0.8049 - val_loss: 0.3923 - val_accuracy: 0.8462\n",
      "Epoch 266/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4323 - accuracy: 0.7926 - val_loss: 0.3939 - val_accuracy: 0.8462\n",
      "Epoch 267/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4300 - accuracy: 0.7909 - val_loss: 0.3927 - val_accuracy: 0.8462\n",
      "Epoch 268/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4304 - accuracy: 0.7873 - val_loss: 0.3967 - val_accuracy: 0.8252\n",
      "Epoch 269/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4331 - accuracy: 0.7803 - val_loss: 0.3941 - val_accuracy: 0.8462\n",
      "Epoch 270/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4310 - accuracy: 0.7909 - val_loss: 0.3942 - val_accuracy: 0.8462\n",
      "Epoch 271/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4300 - accuracy: 0.7979 - val_loss: 0.3940 - val_accuracy: 0.8462\n",
      "Epoch 272/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4304 - accuracy: 0.7979 - val_loss: 0.3965 - val_accuracy: 0.8252\n",
      "Epoch 273/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4333 - accuracy: 0.7891 - val_loss: 0.3933 - val_accuracy: 0.8462\n",
      "Epoch 274/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4305 - accuracy: 0.7891 - val_loss: 0.3947 - val_accuracy: 0.8462\n",
      "Epoch 275/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4295 - accuracy: 0.7979 - val_loss: 0.3946 - val_accuracy: 0.8462\n",
      "Epoch 276/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4301 - accuracy: 0.8032 - val_loss: 0.3936 - val_accuracy: 0.8462\n",
      "Epoch 277/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4312 - accuracy: 0.7979 - val_loss: 0.3936 - val_accuracy: 0.8462\n",
      "Epoch 278/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4301 - accuracy: 0.7909 - val_loss: 0.3942 - val_accuracy: 0.8462\n",
      "Epoch 279/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4303 - accuracy: 0.8014 - val_loss: 0.3949 - val_accuracy: 0.8462\n",
      "Epoch 280/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4302 - accuracy: 0.7961 - val_loss: 0.3937 - val_accuracy: 0.8462\n",
      "Epoch 281/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4345 - accuracy: 0.7891 - val_loss: 0.3990 - val_accuracy: 0.8252\n",
      "Epoch 282/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4317 - accuracy: 0.7891 - val_loss: 0.3936 - val_accuracy: 0.8462\n",
      "Epoch 283/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4301 - accuracy: 0.8014 - val_loss: 0.3945 - val_accuracy: 0.8462\n",
      "Epoch 284/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4290 - accuracy: 0.8067 - val_loss: 0.3948 - val_accuracy: 0.8462\n",
      "Epoch 285/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4326 - accuracy: 0.7856 - val_loss: 0.3978 - val_accuracy: 0.8252\n",
      "Epoch 286/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4311 - accuracy: 0.7873 - val_loss: 0.3930 - val_accuracy: 0.8462\n",
      "Epoch 287/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4284 - accuracy: 0.7944 - val_loss: 0.3994 - val_accuracy: 0.8252\n",
      "Epoch 288/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4326 - accuracy: 0.7838 - val_loss: 0.3922 - val_accuracy: 0.8531\n",
      "Epoch 289/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4314 - accuracy: 0.7944 - val_loss: 0.3966 - val_accuracy: 0.8252\n",
      "Epoch 290/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4303 - accuracy: 0.7803 - val_loss: 0.3962 - val_accuracy: 0.8252\n",
      "Epoch 291/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4292 - accuracy: 0.7909 - val_loss: 0.3949 - val_accuracy: 0.8392\n",
      "Epoch 292/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4291 - accuracy: 0.8032 - val_loss: 0.3936 - val_accuracy: 0.8462\n",
      "Epoch 293/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4300 - accuracy: 0.7909 - val_loss: 0.3977 - val_accuracy: 0.8252\n",
      "Epoch 294/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4302 - accuracy: 0.7891 - val_loss: 0.3945 - val_accuracy: 0.8462\n",
      "Epoch 295/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4313 - accuracy: 0.7961 - val_loss: 0.3923 - val_accuracy: 0.8462\n",
      "Epoch 296/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4286 - accuracy: 0.7944 - val_loss: 0.3958 - val_accuracy: 0.8252\n",
      "Epoch 297/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4323 - accuracy: 0.7979 - val_loss: 0.3931 - val_accuracy: 0.8462\n",
      "Epoch 298/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4283 - accuracy: 0.7909 - val_loss: 0.3974 - val_accuracy: 0.8252\n",
      "Epoch 299/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4294 - accuracy: 0.8049 - val_loss: 0.3968 - val_accuracy: 0.8252\n",
      "Epoch 300/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4352 - accuracy: 0.7821 - val_loss: 0.3932 - val_accuracy: 0.8462\n",
      "Epoch 301/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4289 - accuracy: 0.7979 - val_loss: 0.3981 - val_accuracy: 0.8322\n",
      "Epoch 302/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4305 - accuracy: 0.7856 - val_loss: 0.3934 - val_accuracy: 0.8462\n",
      "Epoch 303/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4312 - accuracy: 0.7891 - val_loss: 0.3978 - val_accuracy: 0.8322\n",
      "Epoch 304/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4309 - accuracy: 0.7996 - val_loss: 0.3950 - val_accuracy: 0.8392\n",
      "Epoch 305/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4285 - accuracy: 0.8032 - val_loss: 0.3949 - val_accuracy: 0.8462\n",
      "Epoch 306/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4304 - accuracy: 0.7979 - val_loss: 0.3945 - val_accuracy: 0.8462\n",
      "Epoch 307/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4360 - accuracy: 0.7838 - val_loss: 0.3985 - val_accuracy: 0.8322\n",
      "Epoch 308/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4291 - accuracy: 0.8014 - val_loss: 0.3926 - val_accuracy: 0.8462\n",
      "Epoch 309/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4341 - accuracy: 0.7821 - val_loss: 0.3973 - val_accuracy: 0.8252\n",
      "Epoch 310/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4295 - accuracy: 0.7909 - val_loss: 0.3932 - val_accuracy: 0.8462\n",
      "Epoch 311/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4294 - accuracy: 0.7996 - val_loss: 0.3945 - val_accuracy: 0.8462\n",
      "Epoch 312/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4294 - accuracy: 0.8014 - val_loss: 0.3950 - val_accuracy: 0.8462\n",
      "Epoch 313/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4311 - accuracy: 0.7961 - val_loss: 0.3968 - val_accuracy: 0.8252\n",
      "Epoch 314/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4293 - accuracy: 0.8049 - val_loss: 0.3960 - val_accuracy: 0.8462\n",
      "Epoch 315/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4289 - accuracy: 0.8032 - val_loss: 0.3942 - val_accuracy: 0.8462\n",
      "Epoch 316/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4296 - accuracy: 0.7926 - val_loss: 0.3939 - val_accuracy: 0.8462\n",
      "Epoch 317/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4315 - accuracy: 0.7786 - val_loss: 0.3944 - val_accuracy: 0.8462\n",
      "Epoch 318/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4309 - accuracy: 0.8014 - val_loss: 0.3950 - val_accuracy: 0.8462\n",
      "Epoch 319/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4290 - accuracy: 0.7909 - val_loss: 0.3943 - val_accuracy: 0.8462\n",
      "Epoch 320/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4295 - accuracy: 0.7996 - val_loss: 0.3994 - val_accuracy: 0.8252\n",
      "Epoch 321/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4318 - accuracy: 0.8067 - val_loss: 0.3928 - val_accuracy: 0.8462\n",
      "Epoch 322/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4288 - accuracy: 0.8049 - val_loss: 0.3974 - val_accuracy: 0.8252\n",
      "Epoch 323/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4287 - accuracy: 0.8049 - val_loss: 0.3943 - val_accuracy: 0.8462\n",
      "Epoch 324/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4302 - accuracy: 0.7909 - val_loss: 0.3979 - val_accuracy: 0.8252\n",
      "Epoch 325/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4302 - accuracy: 0.7926 - val_loss: 0.3951 - val_accuracy: 0.8462\n",
      "Epoch 326/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4302 - accuracy: 0.7979 - val_loss: 0.3972 - val_accuracy: 0.8252\n",
      "Epoch 327/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4296 - accuracy: 0.7996 - val_loss: 0.3933 - val_accuracy: 0.8462\n",
      "Epoch 328/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4321 - accuracy: 0.7926 - val_loss: 0.3966 - val_accuracy: 0.8252\n",
      "Epoch 329/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4314 - accuracy: 0.7821 - val_loss: 0.3944 - val_accuracy: 0.8462\n",
      "Epoch 330/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4314 - accuracy: 0.7891 - val_loss: 0.3980 - val_accuracy: 0.8252\n",
      "Epoch 331/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4296 - accuracy: 0.7873 - val_loss: 0.3944 - val_accuracy: 0.8462\n",
      "Epoch 332/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4289 - accuracy: 0.7926 - val_loss: 0.3964 - val_accuracy: 0.8252\n",
      "Epoch 333/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4282 - accuracy: 0.8014 - val_loss: 0.3949 - val_accuracy: 0.8462\n",
      "Epoch 334/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4288 - accuracy: 0.7926 - val_loss: 0.3965 - val_accuracy: 0.8252\n",
      "Epoch 335/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4283 - accuracy: 0.7961 - val_loss: 0.3940 - val_accuracy: 0.8462\n",
      "Epoch 336/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4288 - accuracy: 0.7979 - val_loss: 0.3967 - val_accuracy: 0.8252\n",
      "Epoch 337/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4293 - accuracy: 0.7944 - val_loss: 0.3962 - val_accuracy: 0.8252\n",
      "Epoch 338/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4300 - accuracy: 0.7996 - val_loss: 0.3946 - val_accuracy: 0.8462\n",
      "Epoch 339/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4300 - accuracy: 0.7856 - val_loss: 0.3958 - val_accuracy: 0.8462\n",
      "Epoch 340/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4283 - accuracy: 0.8067 - val_loss: 0.3955 - val_accuracy: 0.8462\n",
      "Epoch 341/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4311 - accuracy: 0.8014 - val_loss: 0.3942 - val_accuracy: 0.8462\n",
      "Epoch 342/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4310 - accuracy: 0.8014 - val_loss: 0.3965 - val_accuracy: 0.8392\n",
      "Epoch 343/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4291 - accuracy: 0.7961 - val_loss: 0.3957 - val_accuracy: 0.8462\n",
      "Epoch 344/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4310 - accuracy: 0.7996 - val_loss: 0.3934 - val_accuracy: 0.8462\n",
      "Epoch 345/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4360 - accuracy: 0.7961 - val_loss: 0.3962 - val_accuracy: 0.8252\n",
      "Epoch 346/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4304 - accuracy: 0.7944 - val_loss: 0.3989 - val_accuracy: 0.8322\n",
      "Epoch 347/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4324 - accuracy: 0.7979 - val_loss: 0.3931 - val_accuracy: 0.8462\n",
      "Epoch 348/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4292 - accuracy: 0.7979 - val_loss: 0.3973 - val_accuracy: 0.8252\n",
      "Epoch 349/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4283 - accuracy: 0.7979 - val_loss: 0.3940 - val_accuracy: 0.8462\n",
      "Epoch 350/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4282 - accuracy: 0.7996 - val_loss: 0.3960 - val_accuracy: 0.8322\n",
      "Epoch 351/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4279 - accuracy: 0.8067 - val_loss: 0.3946 - val_accuracy: 0.8462\n",
      "Epoch 352/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4287 - accuracy: 0.7873 - val_loss: 0.3945 - val_accuracy: 0.8462\n",
      "Epoch 353/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4331 - accuracy: 0.7838 - val_loss: 0.3930 - val_accuracy: 0.8531\n",
      "Epoch 354/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4311 - accuracy: 0.7891 - val_loss: 0.3953 - val_accuracy: 0.8462\n",
      "Epoch 355/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4312 - accuracy: 0.8067 - val_loss: 0.3938 - val_accuracy: 0.8462\n",
      "Epoch 356/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4283 - accuracy: 0.7996 - val_loss: 0.3972 - val_accuracy: 0.8252\n",
      "Epoch 357/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4287 - accuracy: 0.7961 - val_loss: 0.3958 - val_accuracy: 0.8462\n",
      "Epoch 358/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4284 - accuracy: 0.8049 - val_loss: 0.3962 - val_accuracy: 0.8252\n",
      "Epoch 359/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4312 - accuracy: 0.7926 - val_loss: 0.3954 - val_accuracy: 0.8462\n",
      "Epoch 360/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4294 - accuracy: 0.7996 - val_loss: 0.3978 - val_accuracy: 0.8252\n",
      "Epoch 361/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4296 - accuracy: 0.7838 - val_loss: 0.3936 - val_accuracy: 0.8462\n",
      "Epoch 362/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4285 - accuracy: 0.8049 - val_loss: 0.3991 - val_accuracy: 0.8252\n",
      "Epoch 363/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4278 - accuracy: 0.8084 - val_loss: 0.3949 - val_accuracy: 0.8462\n",
      "Epoch 364/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4289 - accuracy: 0.7944 - val_loss: 0.3959 - val_accuracy: 0.8392\n",
      "Epoch 365/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4297 - accuracy: 0.7979 - val_loss: 0.3983 - val_accuracy: 0.8252\n",
      "Epoch 366/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4303 - accuracy: 0.7996 - val_loss: 0.3930 - val_accuracy: 0.8531\n",
      "Epoch 367/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4311 - accuracy: 0.7926 - val_loss: 0.4026 - val_accuracy: 0.8252\n",
      "Epoch 368/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4288 - accuracy: 0.7926 - val_loss: 0.3941 - val_accuracy: 0.8462\n",
      "Epoch 369/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4327 - accuracy: 0.7856 - val_loss: 0.3951 - val_accuracy: 0.8462\n",
      "Epoch 370/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4293 - accuracy: 0.8014 - val_loss: 0.3990 - val_accuracy: 0.8252\n",
      "Epoch 371/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4277 - accuracy: 0.8032 - val_loss: 0.3943 - val_accuracy: 0.8462\n",
      "Epoch 372/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4312 - accuracy: 0.7803 - val_loss: 0.3965 - val_accuracy: 0.8252\n",
      "Epoch 373/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4285 - accuracy: 0.8067 - val_loss: 0.3967 - val_accuracy: 0.8252\n",
      "Epoch 374/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4314 - accuracy: 0.7996 - val_loss: 0.3969 - val_accuracy: 0.8252\n",
      "Epoch 375/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4308 - accuracy: 0.8032 - val_loss: 0.3937 - val_accuracy: 0.8462\n",
      "Epoch 376/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4299 - accuracy: 0.7944 - val_loss: 0.3956 - val_accuracy: 0.8462\n",
      "Epoch 377/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4280 - accuracy: 0.7909 - val_loss: 0.3956 - val_accuracy: 0.8462\n",
      "Epoch 378/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4290 - accuracy: 0.7944 - val_loss: 0.3968 - val_accuracy: 0.8252\n",
      "Epoch 379/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4287 - accuracy: 0.8049 - val_loss: 0.3996 - val_accuracy: 0.8252\n",
      "Epoch 380/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4291 - accuracy: 0.7926 - val_loss: 0.3938 - val_accuracy: 0.8462\n",
      "Epoch 381/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4285 - accuracy: 0.8014 - val_loss: 0.3975 - val_accuracy: 0.8252\n",
      "Epoch 382/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4285 - accuracy: 0.7873 - val_loss: 0.3959 - val_accuracy: 0.8462\n",
      "Epoch 383/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4290 - accuracy: 0.7944 - val_loss: 0.3991 - val_accuracy: 0.8322\n",
      "Epoch 384/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4322 - accuracy: 0.7786 - val_loss: 0.3935 - val_accuracy: 0.8462\n",
      "Epoch 385/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4267 - accuracy: 0.7979 - val_loss: 0.3977 - val_accuracy: 0.8252\n",
      "Epoch 386/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4282 - accuracy: 0.8084 - val_loss: 0.3959 - val_accuracy: 0.8462\n",
      "Epoch 387/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4301 - accuracy: 0.8067 - val_loss: 0.3995 - val_accuracy: 0.8252\n",
      "Epoch 388/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4299 - accuracy: 0.7891 - val_loss: 0.3965 - val_accuracy: 0.8462\n",
      "Epoch 389/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4284 - accuracy: 0.8084 - val_loss: 0.3954 - val_accuracy: 0.8462\n",
      "Epoch 390/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4279 - accuracy: 0.7891 - val_loss: 0.3949 - val_accuracy: 0.8462\n",
      "Epoch 391/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4294 - accuracy: 0.8049 - val_loss: 0.3970 - val_accuracy: 0.8252\n",
      "Epoch 392/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4274 - accuracy: 0.7996 - val_loss: 0.3941 - val_accuracy: 0.8462\n",
      "Epoch 393/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4286 - accuracy: 0.7961 - val_loss: 0.4001 - val_accuracy: 0.8252\n",
      "Epoch 394/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4351 - accuracy: 0.7909 - val_loss: 0.3953 - val_accuracy: 0.8462\n",
      "Epoch 395/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4276 - accuracy: 0.7996 - val_loss: 0.4013 - val_accuracy: 0.8252\n",
      "Epoch 396/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4321 - accuracy: 0.7803 - val_loss: 0.3948 - val_accuracy: 0.8462\n",
      "Epoch 397/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4281 - accuracy: 0.8067 - val_loss: 0.3974 - val_accuracy: 0.8252\n",
      "Epoch 398/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4276 - accuracy: 0.8049 - val_loss: 0.3950 - val_accuracy: 0.8462\n",
      "Epoch 399/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4308 - accuracy: 0.8014 - val_loss: 0.3962 - val_accuracy: 0.8252\n",
      "Epoch 400/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4285 - accuracy: 0.8032 - val_loss: 0.3940 - val_accuracy: 0.8462\n",
      "Epoch 401/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4268 - accuracy: 0.8049 - val_loss: 0.3971 - val_accuracy: 0.8252\n",
      "Epoch 402/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4285 - accuracy: 0.8049 - val_loss: 0.3984 - val_accuracy: 0.8252\n",
      "Epoch 403/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4286 - accuracy: 0.7909 - val_loss: 0.3958 - val_accuracy: 0.8462\n",
      "Epoch 404/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4302 - accuracy: 0.7961 - val_loss: 0.3953 - val_accuracy: 0.8462\n",
      "Epoch 405/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4301 - accuracy: 0.7891 - val_loss: 0.3962 - val_accuracy: 0.8462\n",
      "Epoch 406/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4293 - accuracy: 0.7996 - val_loss: 0.3980 - val_accuracy: 0.8252\n",
      "Epoch 407/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4264 - accuracy: 0.8032 - val_loss: 0.3941 - val_accuracy: 0.8462\n",
      "Epoch 408/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4291 - accuracy: 0.7891 - val_loss: 0.3974 - val_accuracy: 0.8252\n",
      "Epoch 409/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4281 - accuracy: 0.8032 - val_loss: 0.3948 - val_accuracy: 0.8462\n",
      "Epoch 410/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4276 - accuracy: 0.8014 - val_loss: 0.3960 - val_accuracy: 0.8462\n",
      "Epoch 411/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4276 - accuracy: 0.7979 - val_loss: 0.3960 - val_accuracy: 0.8462\n",
      "Epoch 412/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4277 - accuracy: 0.8084 - val_loss: 0.3988 - val_accuracy: 0.8252\n",
      "Epoch 413/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4287 - accuracy: 0.8032 - val_loss: 0.3950 - val_accuracy: 0.8462\n",
      "Epoch 414/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4270 - accuracy: 0.7996 - val_loss: 0.3976 - val_accuracy: 0.8252\n",
      "Epoch 415/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4294 - accuracy: 0.8067 - val_loss: 0.3966 - val_accuracy: 0.8462\n",
      "Epoch 416/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4310 - accuracy: 0.8032 - val_loss: 0.3946 - val_accuracy: 0.8462\n",
      "Epoch 417/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4276 - accuracy: 0.8084 - val_loss: 0.3989 - val_accuracy: 0.8252\n",
      "Epoch 418/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4284 - accuracy: 0.7996 - val_loss: 0.3962 - val_accuracy: 0.8462\n",
      "Epoch 419/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4281 - accuracy: 0.8049 - val_loss: 0.3975 - val_accuracy: 0.8252\n",
      "Epoch 420/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4274 - accuracy: 0.7961 - val_loss: 0.3957 - val_accuracy: 0.8462\n",
      "Epoch 421/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4272 - accuracy: 0.8084 - val_loss: 0.3973 - val_accuracy: 0.8322\n",
      "Epoch 422/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4289 - accuracy: 0.8032 - val_loss: 0.3953 - val_accuracy: 0.8462\n",
      "Epoch 423/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4273 - accuracy: 0.8014 - val_loss: 0.3973 - val_accuracy: 0.8252\n",
      "Epoch 424/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4287 - accuracy: 0.8014 - val_loss: 0.3951 - val_accuracy: 0.8462\n",
      "Epoch 425/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4280 - accuracy: 0.7961 - val_loss: 0.3962 - val_accuracy: 0.8462\n",
      "Epoch 426/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4323 - accuracy: 0.8102 - val_loss: 0.3956 - val_accuracy: 0.8462\n",
      "Epoch 427/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4292 - accuracy: 0.7996 - val_loss: 0.3951 - val_accuracy: 0.8462\n",
      "Epoch 428/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4274 - accuracy: 0.8084 - val_loss: 0.3977 - val_accuracy: 0.8252\n",
      "Epoch 429/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4296 - accuracy: 0.7961 - val_loss: 0.3946 - val_accuracy: 0.8462\n",
      "Epoch 430/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4270 - accuracy: 0.7996 - val_loss: 0.3990 - val_accuracy: 0.8252\n",
      "Epoch 431/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4303 - accuracy: 0.8049 - val_loss: 0.3976 - val_accuracy: 0.8252\n",
      "Epoch 432/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4300 - accuracy: 0.7944 - val_loss: 0.3969 - val_accuracy: 0.8322\n",
      "Epoch 433/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4277 - accuracy: 0.8067 - val_loss: 0.3976 - val_accuracy: 0.8322\n",
      "Epoch 434/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4280 - accuracy: 0.7979 - val_loss: 0.3962 - val_accuracy: 0.8462\n",
      "Epoch 435/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4282 - accuracy: 0.7979 - val_loss: 0.3968 - val_accuracy: 0.8462\n",
      "Epoch 436/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4280 - accuracy: 0.7996 - val_loss: 0.3963 - val_accuracy: 0.8462\n",
      "Epoch 437/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4290 - accuracy: 0.8014 - val_loss: 0.3977 - val_accuracy: 0.8252\n",
      "Epoch 438/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4282 - accuracy: 0.8067 - val_loss: 0.3965 - val_accuracy: 0.8322\n",
      "Epoch 439/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4309 - accuracy: 0.7873 - val_loss: 0.3946 - val_accuracy: 0.8462\n",
      "Epoch 440/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4304 - accuracy: 0.7891 - val_loss: 0.3954 - val_accuracy: 0.8462\n",
      "Epoch 441/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4270 - accuracy: 0.8049 - val_loss: 0.3978 - val_accuracy: 0.8322\n",
      "Epoch 442/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4269 - accuracy: 0.8049 - val_loss: 0.3957 - val_accuracy: 0.8462\n",
      "Epoch 443/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4272 - accuracy: 0.8102 - val_loss: 0.3956 - val_accuracy: 0.8462\n",
      "Epoch 444/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4271 - accuracy: 0.8014 - val_loss: 0.3983 - val_accuracy: 0.8252\n",
      "Epoch 445/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4271 - accuracy: 0.8032 - val_loss: 0.3955 - val_accuracy: 0.8462\n",
      "Epoch 446/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4274 - accuracy: 0.8102 - val_loss: 0.3959 - val_accuracy: 0.8462\n",
      "Epoch 447/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4301 - accuracy: 0.7979 - val_loss: 0.3955 - val_accuracy: 0.8462\n",
      "Epoch 448/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4283 - accuracy: 0.7961 - val_loss: 0.3955 - val_accuracy: 0.8462\n",
      "Epoch 449/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4271 - accuracy: 0.7961 - val_loss: 0.3967 - val_accuracy: 0.8462\n",
      "Epoch 450/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4280 - accuracy: 0.8084 - val_loss: 0.3959 - val_accuracy: 0.8462\n",
      "Epoch 451/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4304 - accuracy: 0.7979 - val_loss: 0.3951 - val_accuracy: 0.8462\n",
      "Epoch 452/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4281 - accuracy: 0.8032 - val_loss: 0.3990 - val_accuracy: 0.8252\n",
      "Epoch 453/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4303 - accuracy: 0.7961 - val_loss: 0.3948 - val_accuracy: 0.8462\n",
      "Epoch 454/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4269 - accuracy: 0.8014 - val_loss: 0.3994 - val_accuracy: 0.8252\n",
      "Epoch 455/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4282 - accuracy: 0.7979 - val_loss: 0.3959 - val_accuracy: 0.8462\n",
      "Epoch 456/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4277 - accuracy: 0.8084 - val_loss: 0.3953 - val_accuracy: 0.8462\n",
      "Epoch 457/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4281 - accuracy: 0.7926 - val_loss: 0.3945 - val_accuracy: 0.8462\n",
      "Epoch 458/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4275 - accuracy: 0.8049 - val_loss: 0.3965 - val_accuracy: 0.8462\n",
      "Epoch 459/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4275 - accuracy: 0.8049 - val_loss: 0.3984 - val_accuracy: 0.8252\n",
      "Epoch 460/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4280 - accuracy: 0.8067 - val_loss: 0.3961 - val_accuracy: 0.8462\n",
      "Epoch 461/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4283 - accuracy: 0.7944 - val_loss: 0.3972 - val_accuracy: 0.8322\n",
      "Epoch 462/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4268 - accuracy: 0.8014 - val_loss: 0.3951 - val_accuracy: 0.8462\n",
      "Epoch 463/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4276 - accuracy: 0.7926 - val_loss: 0.3996 - val_accuracy: 0.8252\n",
      "Epoch 464/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4285 - accuracy: 0.7979 - val_loss: 0.3954 - val_accuracy: 0.8462\n",
      "Epoch 465/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4285 - accuracy: 0.8067 - val_loss: 0.3966 - val_accuracy: 0.8462\n",
      "Epoch 466/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4282 - accuracy: 0.7996 - val_loss: 0.3989 - val_accuracy: 0.8252\n",
      "Epoch 467/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4289 - accuracy: 0.7979 - val_loss: 0.3942 - val_accuracy: 0.8462\n",
      "Epoch 468/500\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.4305 - accuracy: 0.8032 - val_loss: 0.3950 - val_accuracy: 0.8462\n",
      "Epoch 469/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4278 - accuracy: 0.8032 - val_loss: 0.3995 - val_accuracy: 0.8252\n",
      "Epoch 470/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4271 - accuracy: 0.7996 - val_loss: 0.3943 - val_accuracy: 0.8462\n",
      "Epoch 471/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4280 - accuracy: 0.8102 - val_loss: 0.3993 - val_accuracy: 0.8252\n",
      "Epoch 472/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4319 - accuracy: 0.7838 - val_loss: 0.3968 - val_accuracy: 0.8462\n",
      "Epoch 473/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4280 - accuracy: 0.8032 - val_loss: 0.4003 - val_accuracy: 0.8322\n",
      "Epoch 474/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4307 - accuracy: 0.7944 - val_loss: 0.3960 - val_accuracy: 0.8462\n",
      "Epoch 475/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4276 - accuracy: 0.7961 - val_loss: 0.3961 - val_accuracy: 0.8462\n",
      "Epoch 476/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4274 - accuracy: 0.7944 - val_loss: 0.3969 - val_accuracy: 0.8322\n",
      "Epoch 477/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4266 - accuracy: 0.8084 - val_loss: 0.3978 - val_accuracy: 0.8322\n",
      "Epoch 478/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4292 - accuracy: 0.7996 - val_loss: 0.3953 - val_accuracy: 0.8462\n",
      "Epoch 479/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4253 - accuracy: 0.8067 - val_loss: 0.3989 - val_accuracy: 0.8252\n",
      "Epoch 480/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4277 - accuracy: 0.8049 - val_loss: 0.4006 - val_accuracy: 0.8252\n",
      "Epoch 481/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4257 - accuracy: 0.8067 - val_loss: 0.3957 - val_accuracy: 0.8462\n",
      "Epoch 482/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4269 - accuracy: 0.7961 - val_loss: 0.3964 - val_accuracy: 0.8462\n",
      "Epoch 483/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4275 - accuracy: 0.8049 - val_loss: 0.4000 - val_accuracy: 0.8252\n",
      "Epoch 484/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4257 - accuracy: 0.8067 - val_loss: 0.3946 - val_accuracy: 0.8462\n",
      "Epoch 485/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4287 - accuracy: 0.7979 - val_loss: 0.3970 - val_accuracy: 0.8322\n",
      "Epoch 486/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4287 - accuracy: 0.7856 - val_loss: 0.3979 - val_accuracy: 0.8322\n",
      "Epoch 487/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4289 - accuracy: 0.7944 - val_loss: 0.3967 - val_accuracy: 0.8462\n",
      "Epoch 488/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4287 - accuracy: 0.7926 - val_loss: 0.3963 - val_accuracy: 0.8462\n",
      "Epoch 489/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4281 - accuracy: 0.7909 - val_loss: 0.3987 - val_accuracy: 0.8322\n",
      "Epoch 490/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4328 - accuracy: 0.7926 - val_loss: 0.3945 - val_accuracy: 0.8531\n",
      "Epoch 491/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4276 - accuracy: 0.7891 - val_loss: 0.3997 - val_accuracy: 0.8322\n",
      "Epoch 492/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4278 - accuracy: 0.7979 - val_loss: 0.3962 - val_accuracy: 0.8462\n",
      "Epoch 493/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4292 - accuracy: 0.7891 - val_loss: 0.3960 - val_accuracy: 0.8462\n",
      "Epoch 494/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4283 - accuracy: 0.8032 - val_loss: 0.3955 - val_accuracy: 0.8462\n",
      "Epoch 495/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4269 - accuracy: 0.8067 - val_loss: 0.3971 - val_accuracy: 0.8322\n",
      "Epoch 496/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4303 - accuracy: 0.7979 - val_loss: 0.3945 - val_accuracy: 0.8462\n",
      "Epoch 497/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4334 - accuracy: 0.7926 - val_loss: 0.3984 - val_accuracy: 0.8322\n",
      "Epoch 498/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4301 - accuracy: 0.7891 - val_loss: 0.3942 - val_accuracy: 0.8462\n",
      "Epoch 499/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4351 - accuracy: 0.7909 - val_loss: 0.3958 - val_accuracy: 0.8462\n",
      "Epoch 500/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4277 - accuracy: 0.8014 - val_loss: 0.3979 - val_accuracy: 0.8322\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4121 - accuracy: 0.7933\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Your submission was successfully saved!\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 100)               700       \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,721\n",
      "Trainable params: 1,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Sie können von folgendem Code starten um das Netzwerk zu definieren, füllen Sie die ...\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# make Network\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='sigmoid', batch_input_shape=(None, 6))) #We have 4 input features\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Read Testfiles\n",
    "train_val = pd.read_csv('titanic/train.csv')\n",
    "titanic_test_data = pd.read_csv('titanic/test.csv')\n",
    "\n",
    "train_val[\"Age\"].fillna(train_val[\"Age\"].median(skipna=True), inplace=True)\n",
    "test_data[\"Age\"].fillna(train_val[\"Age\"].median(skipna=True), inplace=True)\n",
    "\n",
    "train, validate = train_test_split(train_val, test_size=0.2)\n",
    "\n",
    "# train Data\n",
    "pClassAsNum = pd.get_dummies(train['Pclass'])\n",
    "sexAsNum = pd.get_dummies(train['Sex'])\n",
    "train[\"Age\"].fillna(train[\"Age\"].median(skipna=True), inplace=True)\n",
    "\n",
    "toFit = pd.concat([pClassAsNum, sexAsNum, train[\"Age\"]], axis=1)\n",
    "# print(toFit)\n",
    "history = model.fit(toFit, train['Survived'], validation_split=0.2, epochs=100)\n",
    "\n",
    "# validate Data\n",
    "pClassAsNumValidate = pd.get_dummies(validate['Pclass'])\n",
    "sexAsNumValidate = pd.get_dummies(validate['Sex'])\n",
    "validate[\"Age\"].fillna(validate[\"Age\"].median(skipna=True), inplace=True)\n",
    "\n",
    "toValidate = pd.concat([pClassAsNumValidate, sexAsNumValidate, validate[\"Age\"]], axis=1)\n",
    "\n",
    "\n",
    "# prediction Data\n",
    "pClassAsNumTitanic = pd.get_dummies(titanic_test_data['Pclass'])\n",
    "sexAsNumTitanic = pd.get_dummies(titanic_test_data['Sex'])\n",
    "titanic_test_data[\"Age\"].fillna(titanic_test_data[\"Age\"].median(skipna=True), inplace=True)\n",
    "titanicTestDataShort = pd.concat([pClassAsNumTitanic, sexAsNumTitanic, titanic_test_data[\"Age\"]], axis=1)\n",
    "\n",
    "# predict\n",
    "model.evaluate(toValidate, validate[\"Survived\"])\n",
    "pred_survived = model.predict(np.asarray(titanicTestDataShort).astype(float))\n",
    "for i in range(len(pred_survived)):\n",
    "    if pred_survived[i] > 0.49:\n",
    "        pred_survived[i] = 1\n",
    "    else:\n",
    "        pred_survived[i] = 0\n",
    "# print(pred_survived)\n",
    "output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': pred_survived.astype(int).flatten()})\n",
    "# print(output)\n",
    "output.to_csv('my_submission_v2.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "9M8BQdo5wWgs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotten\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss Kurve')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('value')\n",
    "#plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "colab": {
   "name": "ki_ml_blatt_1.ipynb",
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}